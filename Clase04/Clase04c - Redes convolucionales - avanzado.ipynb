{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture09_Profe.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"k3DsP3PLnIAq"},"source":["# **Lecture 09 - Introducción a las CNNs parte 2 - Arquitecturas de CNNs**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- Dropout 2D y batchnorm\n","- Arquitecturas comunes \n","  - VGG16 (simple, CNN profunda)\n","  - ResNet y skip connections\n","- Reemplazando Max-Pooling con capas convolucionales\n","- Capas convolucionales en lugar de completamente conectadas\n","- Transfer learning\n"]},{"cell_type":"markdown","metadata":{"id":"3c9aQMn8ox5O"},"source":["# **Controlar el tamaño de salida además del paso**\n","\n","- **Padding (controla el tamaño de la salida junto con stride)**\n","- Dropout 2D y batchnorm\n","- Arquitecturas comunes \n","  - VGG16 (simple, CNN profunda)\n","  - ResNet y skip connections\n","- Reemplazando Max-Pooling con capas convolucionales\n","- Capas convolucionales en lugar de completamente conectadas\n","- Transfer learning\n"]},{"cell_type":"markdown","metadata":{"id":"8onLqoIao7jw"},"source":["# **Relleno**\n","\n","![](https://drive.google.com/uc?id=1PMuSf4cb14P0Q4Sa5zXcHjFdME1Woq3L)"]},{"cell_type":"markdown","metadata":{"id":"aMrI8kF6pbQr"},"source":["![](https://drive.google.com/uc?id=1GdduJrY3ipz5lNFadpneFgrXtGKD4_Yf)\n","\n","### **Altamente recomendado:**\n","Dumoulin, Vincent, and Francesco Visin. \"A guide to convolution arithmetic for deep learning.\" arXiv preprint arXiv:1603.07285 (2016).\n","\n","https://arxiv.org/abs/1603.07285\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lusaGlgQpx8E"},"source":["# **Jerga de Padding**\n","\n","Convolución \"valid\": Sin padding (el mapa de características se reduce)\n","\n","Convolución \"same\": Padding tal que el tamaño de salida es igual al tamaño de entrada\n","\n","Convenciones comunes de tamaño de kernel:\n","\n","3x3, 5x5, 7x7 (a veces 1x1 en capas posteriores para reducir canales)"]},{"cell_type":"markdown","metadata":{"id":"UBJgUguGqnzJ"},"source":["# **Padding**\n","\n","#$o = [\\frac{i+2p-k}{s}]+1$\n","\n","Suponga que desea utilizar una operación convolucional con paso 1 y mantener las dimensiones de entrada en el mapa de características de salida:\n","\n","¿Cuánto padding se necesita para que el tamaño del mapa de características sea igual al tamaño de entrada?\n","\n","# $o=i+2p-k+1$\n","# $\\Leftrightarrow p = \\frac{o-i+k-1}{2}$\n","# $\\Leftrightarrow p = \\frac{k-1}{2}$"]},{"cell_type":"markdown","metadata":{"id":"Ev1b-7isroHa"},"source":["# **Padding**\n","\n","# $o=i+2p-k+1$\n","# $\\Leftrightarrow p = \\frac{o-i+k-1}{2}$\n","# $\\Leftrightarrow p = \\frac{k-1}{2}$\n","\n","Probablemente explica por qué las convenciones de tamaño de kernel comunes son 3x3, 5x5, 7x7 (a veces 1x1 en capas posteriores para reducir canales)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cSFGdeg0r167"},"source":["# **Conceptos familiares ahora en 2D**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- **Dropout 2D y batchnorm**\n","- Arquitecturas comunes \n","  - VGG16 (simple, CNN profunda)\n","  - ResNet y skip connections\n","- Reemplazando Max-Pooling con capas convolucionales\n","- Capas convolucionales en lugar de completamente conectadas\n","- Transfer learning"]},{"cell_type":"markdown","metadata":{"id":"cWCN_w3vr8vS"},"source":["# ** Dropout 2D**\n","\n","- Problema con el dropout regular y las CNN: Es probable que los píxeles adyacentes estén muy correlacionados (por lo tanto, es posible que no ayuden a reducir la \"dependencia\" tanto como se pretendía originalmente con el dropout)\n","\n","- Por lo tanto, puede ser mejor eliminar mapas de características completos.\n","\n","**La idea proviene de**\n","Tompson, Jonathan, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler.\n","\"Efficient object localization using convolutional networks.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 648-656. 2015.\n","\n","https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Tompson_Efficient_Object_Localization_2015_CVPR_paper.html\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4HN-3w7bscbf"},"source":["# **Dropout 2D**\n","\n","- Dropout2d descartará mapas de características completos (canales)\n","\n","![](https://drive.google.com/uc?id=1N5lo11Y8_4hSvcRARFFsSj9S6CmCX6vq)\n"]},{"cell_type":"markdown","metadata":{"id":"HnKETvy2sy9t"},"source":["# **BatchNorm 2D**\n","\n","![](https://drive.google.com/uc?id=1UhneaS40Ag_FeaYCP-4UZLTkmGca8eHX)\n","\n","Fuente: https://pytorch.org/docs/stable/nn.html\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zF1LY03ztEaI"},"source":["# **BatchNorm 2D**\n","\n","![](https://drive.google.com/uc?id=1sTlJd4VKHR5REKiV53iT-YI5i11xMwIj)\n","\n","En BatchNorm2d, la desviación media y estándar se calculan para N * H * W, es decir, sobre la dimensión del canal\n","\n","Fuente: https://pytorch.org/docs/stable/nn.html"]},{"cell_type":"markdown","metadata":{"id":"ksChxoCeuHjx"},"source":["# **BatchNorm 2D**\n","\n","En BatchNorm2d, la desviación media y estándar se calculan para N * H * W, es decir, sobre la dimensión del canal\n","\n","![](https://drive.google.com/uc?id=1oEJ5Rqb8rvzl4_nGJ6blIW46Ryx5wwoJ)"]},{"cell_type":"markdown","metadata":{"id":"qLYYGC8kuXqX"},"source":["# **Revisión de las arquitecturas comunes**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- Dropout 2D y batchnorm\n","- **Arquitecturas comunes** \n","  - VGG16 (simple, CNN profunda)\n","  - ResNet y skip connections\n","- Reemplazando Max-Pooling con capas convolucionales\n","- Capas convolucionales en lugar de completamente conectadas\n","- Transfer learning"]},{"cell_type":"markdown","metadata":{"id":"KMfeXTi6uhOC"},"source":["# **Revisión de las arquitecturas comunes**\n","\n","Discutiremos algunas arquitecturas CNN comunes adicionales ya que el campo evolucionó bastante desde 2012.\n","\n","![](https://drive.google.com/uc?id=1tRUe_2GtI5AzYPxOnEgj36SFU6IS8CQm)\n","\n","Canziani, A., Paszke, A., & Culurciello, E. (2016). An analysis of deep neural network models for practical Figure 1: Top1 vs. network. applications. Single-crop top-1 vali- arXiv preprint arXiv:1605.07678."]},{"cell_type":"markdown","metadata":{"id":"mIcy3PIRu5FT"},"source":["# **Revisión de las arquitecturas comunes**\n","\n","Se discutirán algunas arquitecturas CNN comunes adicionales ya que el campo ha evolucionado bastante desde 2012.\n","\n","![](https://drive.google.com/uc?id=1L3u5_yQ3JRHBhpdlPqPOpgVvmYIXlk3V)\n","\n","Canziani, A., Paszke, A., & Culurciello, E. (2016). An analysis of deep neural network models for practical Figure 1: Top1 vs. network. applications. Single-crop top-1 vali- arXiv preprint arXiv:1605.07678."]},{"cell_type":"markdown","metadata":{"id":"eesx3CnMvdoO"},"source":["# **Agregando más capas**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- Dropout 2D y batchnorm\n","- Arquitecturas comunes \n","  - **VGG16 (simple, CNN profunda)**\n","  - ResNet y skip connections\n","- Reemplazando Max-Pooling con capas convolucionales\n","- Capas convolucionales en lugar de completamente conectadas\n","- Transfer learning\n"]},{"cell_type":"markdown","metadata":{"id":"2UckdLO8xMG8"},"source":["# **Revisión de las arquitecturas comunes**\n","\n","![](https://drive.google.com/uc?id=1yXiPMzSjqVcecxod3g-OVNoVacWejhoE)\n","\n","Canziani, A., Paszke, A., & Culurciello, E. (2016). An analysis of deep neural network models for practical Figure 1: Top1 vs. network. applications. Single-crop top-1 vali- arXiv preprint arXiv:1605.07678."]},{"cell_type":"markdown","metadata":{"id":"TYwf0wAPxdr0"},"source":["# **VGG-16**\n","\n","![](https://drive.google.com/uc?id=1v0uU8qsotJvp1T5_VX1ZseFL9iGtNUsM)\n","\n","###Ventajas:\n","Arquitectura muy simple, conv 3x3, stride = 1, padding \"same\" , MaxPooling de 2x2\n","\n","###Desventaja:\n","Gran cantidad de parámetros\n","y lento (ver diapositiva anterior)\n","\n","Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).\n","\n","https://arxiv.org/abs/1409.1556\n"]},{"cell_type":"markdown","metadata":{"id":"5DeB6BUpySBC"},"source":["# **VGG-16**\n","\n","![](https://drive.google.com/uc?id=1AiD6YeUg8mjQYTrQiXay0pbrK-efDDE3)\n","\n","Visualización desde: https://www.cs.toronto.edu/~frossard/post/vgg16/\n","\n","\n","\n","\n","Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).\n","\n","https://arxiv.org/abs/1409.1556"]},{"cell_type":"markdown","metadata":{"id":"YkrrQuYsypCN"},"source":["# **¿Podemos agregar más capas? CNN con un simple truco**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- Dropout 2D y batchnorm\n","- Arquitecturas comunes \n","  - VGG16 (simple, CNN profunda)\n","  - **ResNet y skip connections**\n","- Reemplazando Max-Pooling con capas convolucionales\n","- Capas convolucionales en lugar de completamente conectadas\n","- Transfer learning\n"]},{"cell_type":"markdown","metadata":{"id":"wDOW79t7y0_G"},"source":["# **Revisión de las arquitecturas comunes**\n","\n","![](https://drive.google.com/uc?id=1vd5N3jEFE7jJppxyRPQZAfhIg2hRol6O)\n","\n","Canziani, A., Paszke, A., & Culurciello, E. (2016). An analysis of deep neural network models for practical Figure 1: Top1 vs. network. applications. Single-crop top-1 vali- arXiv preprint arXiv:1605.07678."]},{"cell_type":"markdown","metadata":{"id":"pB8njKrv075K"},"source":["# **ResNets**\n","\n","He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Deep residual learning for image recognition.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.\n","\n","http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\n","\n","Con su simple truco de permitir omitir conexiones (la posibilidad de aprender funciones de identidad y omitir capas que no son útiles), ResNets permite implementar arquitecturas muy, muy profundas\n","\n","![](https://drive.google.com/uc?id=1N1mgrAUzDfa0NDE6sHmJdEBb3m1fNLjc)"]},{"cell_type":"markdown","metadata":{"id":"8T6LJM9A1-O7"},"source":["# **ResNets**\n","\n","He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Deep residual learning for image recognition.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.\n","\n","http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\n","\n","![](https://drive.google.com/uc?id=1ksnFeaXMLGr-iJ3RRHwkmJ97QooIzPjZ)"]},{"cell_type":"markdown","metadata":{"id":"-XhhtTLZ1bnO"},"source":["# **ResNets**\n","\n","![](https://drive.google.com/uc?id=1_27mb4C9N4kcC30eJPabk6yyg2vP_0gA)\n","\n","### **En general: $a^{(l+2)}=\\sigma (z^{(l+2)} + a^{(l)})$**"]},{"cell_type":"markdown","metadata":{"id":"Kd60Dg0w2Su1"},"source":["# **ResNets**\n","\n","![](https://drive.google.com/uc?id=1_27mb4C9N4kcC30eJPabk6yyg2vP_0gA)\n","\n","![](https://drive.google.com/uc?id=1MeCV0sgihLx3t18QMUfCufEhn_nnyz9Q)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nHxY08EM3iiW"},"source":["# **ResNets**\n","\n","![](https://drive.google.com/uc?id=1_27mb4C9N4kcC30eJPabk6yyg2vP_0gA)\n","\n","![](https://drive.google.com/uc?id=1X3Yd9BCVDFH9Dml_hTB7Bjszv0a-CYEn)"]},{"cell_type":"markdown","metadata":{"id":"-lKnzyK64Ev5"},"source":["# **ResNets**\n","\n","![](https://drive.google.com/uc?id=1Ejf2pjocL6l2DtNbBoe3fLHoewgXrpxd)\n","\n","Bloques residuales alternativos con conexiones de salto, de modo que la entrada pasada a través del atajo se redimensione a las dimensiones de la salida de la ruta principal\n","\n"]},{"cell_type":"markdown","metadata":{"id":"chruvz6z4u1X"},"source":["# **Simplificando CNNs**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- Dropout 2D y batchnorm\n","- Arquitecturas comunes \n","  - VGG16 (simple, CNN profunda)\n","  - ResNet y skip connections\n","- **Reemplazando Max-Pooling con capas convolucionales**\n","- Capas convolucionales en lugar de completamente conectadas\n","- Transfer learning"]},{"cell_type":"markdown","metadata":{"id":"2CAIB8s0480s"},"source":["# **\"Red totalmente convolucional\"**\n","\n","Springenberg, Jost Tobias, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. \"Striving for simplicity: The all convolutional net.\" arXiv preprint arXiv:1412.6806 (2014).\n","\n","https://arxiv.org/abs/1412.6806\n","\n","Idea clave: Reemplace Max Pooling por convoluciones escalonadas (es decir, capas convolucionadas con paso = 2)\n","\n","**Podemos pensar en las \"convoluciones escalonadas\" como agrupaciones que se pueden aprender**"]},{"cell_type":"markdown","metadata":{"id":"qAVMA1eO59nA"},"source":["# **Agrupación promedio global en la última capa**\n","\n","![](https://drive.google.com/uc?id=1r7q35K8W4fUfcbBToJHlQAwFkAEa7UYL)\n","\n","Fuente de la imagen: Singh, Anshuman Vikram. \"Content-based image retrieval using deep learning.\" (2015). http://scholarworks.rit.edu/theses/8828/\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E-295tjg6dmE"},"source":["# **Código de Ejemplo**"]},{"cell_type":"markdown","metadata":{"id":"1CIDyfDq6jrH"},"source":["# **Simplificando CNNs parte 2**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- Dropout 2D y batchnorm\n","- Arquitecturas comunes \n","  - VGG16 (simple, CNN profunda)\n","  - ResNet y skip connections\n","- Reemplazando Max-Pooling con capas convolucionales\n","- **Capas convolucionales en lugar de completamente conectadas**\n","- Transfer learning"]},{"cell_type":"markdown","metadata":{"id":"28fnE87P6udy"},"source":["# **Es posible reemplazar capas completamente conectadas por capas convolucionales**\n","\n","![](https://drive.google.com/uc?id=1sQFpvswQ4iwqzd6-Uvm3w0esyo6-i47Q)\n"]},{"cell_type":"markdown","metadata":{"id":"O3nz_4YK7yV0"},"source":["![](https://drive.google.com/uc?id=1YtG_voLo9v95x6J9pezyQ6rp6oGpMWmY)"]},{"cell_type":"markdown","metadata":{"id":"L9u-l6838DTB"},"source":["![](https://drive.google.com/uc?id=1_qV35tVIfrGx1gjP1JRqimipsk3FQUx1)"]},{"cell_type":"markdown","metadata":{"id":"m2XG14_J8M0_"},"source":["# **Es posible reemplazar capas completamente conectadas por capas convolucionales**\n","\n","![](https://drive.google.com/uc?id=1nIL964JcuR7BVBo1_MTtSQuvoCALokgo)\n"]},{"cell_type":"markdown","metadata":{"id":"7YDRF5I38tAP"},"source":["![](https://drive.google.com/uc?id=1PGMR47t6miW07kBwvTBLGI78wYUFHUh8)\n"]},{"cell_type":"markdown","metadata":{"id":"njJqxka283m_"},"source":["![](https://drive.google.com/uc?id=1QMoOoPrYvT8Fxg6LZSfze9Zsuww_gOM7)"]},{"cell_type":"markdown","metadata":{"id":"KnRvrwCS9DRU"},"source":["![](https://drive.google.com/uc?id=1k1r3svkmfKSjjUkgmKyrW8bgzyFFBYwl)"]},{"cell_type":"markdown","metadata":{"id":"Sdt42j9R9SrQ"},"source":["# **¿Se pueden enseñar trucos nuevos a un perro viejo?**\n","\n","- Padding (controla el tamaño de la salida junto con stride)\n","- Dropout 2D y batchnorm\n","- Arquitecturas comunes \n","  - VGG16 (simple, CNN profunda)\n","  - ResNet y skip connections\n","- Reemplazando Max-Pooling con capas convolucionales\n","- Capas convolucionales en lugar de completamente conectadas\n","- **Transfer learning**"]},{"cell_type":"markdown","metadata":{"id":"k__6ieeR9fYF"},"source":["# **Transferir aprendizaje**\n","\n","- Una técnica que puede ser útil para sus proyectos de clase.\n","- Idea clave:\n","  - Las capas de extracción de características pueden ser útiles en general\n","  - Utilice un modelo previamente entrenado (por ejemplo, previamente entrenado en ImageNet)\n","  - Congelar los pesos: Solo entrene la última capa (o las últimas capas)\n","- Enfoque relacionado: Ajuste, entrene una red previamente entrenada en su conjunto de datos más pequeño\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5wIEwRfh-FwE"},"source":["# **¿Qué capas reemplazar y entrenar?**\n","\n","Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., & Fei-Fei, L. (2014). Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (pp. 1725-1732).\n","\n","https://cs.stanford.edu/people/karpathy/deepvideo/\n","\n","![](https://drive.google.com/uc?id=1QLl7MkKjERs0Gv6a_AyMwmc-m69tMB1M)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aMycCMc_-Ypx"},"source":["# **Transfer learning**\n","\n","![](https://drive.google.com/uc?id=1AiD6YeUg8mjQYTrQiXay0pbrK-efDDE3)\n","\n","Visualización desde: https://www.cs.toronto.edu/~frossard/post/vgg16/\n","\n","Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).\n","\n","https://arxiv.org/abs/1409.1556"]},{"cell_type":"markdown","metadata":{"id":"C02Z9EzP-nre"},"source":["# **Transferir aprendizaje**\n","\n","![](https://drive.google.com/uc?id=1_A_UAsTQ5yqSQMpwrMoSe2jnkWP7nlUF)"]},{"cell_type":"markdown","metadata":{"id":"ORTKYpvf-33i"},"source":["# **Transfer learning**\n","\n","https://pytorch.org/vision/stable/models.html\n","\n","![](https://drive.google.com/uc?id=1X0BjPN4_kw_jIW0KmWIaKNufaKRWvdx2)"]},{"cell_type":"markdown","metadata":{"id":"JRt0M1k5_GNM"},"source":["# **Transfer learning**\n","\n","https://pytorch.org/docs/stable/torchvision/models.html\n","\n","![](https://drive.google.com/uc?id=1Mpie3FjVvYJgD4POL0_Qz2VgT0xMa0_4)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CrcXvC_B_Tqw"},"source":["# **Código de ejemplo de transfer learning**"]}]}